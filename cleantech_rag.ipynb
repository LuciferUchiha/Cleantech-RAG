{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SDS 2024 - Cleantech RAG\n",
        "\n",
        "By [Daniel Perruchoud](https://www.linkedin.com/in/daniel-olivier-perruchoud-799aaa38/) and [George Rowlands](https://www.linkedin.com/in/georgerowlands/)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook delves into the exciting realm of Cleantech using a [dataset of nearly 10,000 news articles from Kaggle](https://www.kaggle.com/datasets/jannalipenkova/cleantech-media-dataset), all centered around the energy sector. We'll embark on a journey that includes data exploration, text preprocessing, and culminates in the creation of a Retrieval-Augmented Generation Pipeline (RAG). This powerful approach empowers us to construct an LLM (Large Language Model) that can intelligently answer user queries, drawing upon the knowledge from our curated news articles.\n",
        "\n",
        "### Why RAG? A Cost-Effective and Dynamic Solution\n",
        "\n",
        "Fine-tuning an LLM can be a resource-intensive and inflexible process. RAG offers a compelling alternative. It leverages a semantic search to pinpoint relevant sections within our news articles that directly address a user's question. These retrieved sections are then provided to the LLM as context, enabling it to deliver informed and insightful responses.\n",
        "\n",
        "![rag_pipline](https://arxiv.org/html/2312.10997v5/extracted/2312.10997v5/images/RAG_case.png)\n",
        "\n",
        "### Setup\n",
        "\n",
        "To run this notebook we recommend downloading the provided [GitHub repository](https://github.com/LuciferUchiha/sds2024-cleantech-rag) and opening this notebook in [Google Colab](https://colab.research.google.com/). To ensure a smooth experience, you'll need:\n",
        "\n",
        "- An OpenAI API key for GPT-4 and embedding models\n",
        "- A Google account for Google Colab\n",
        "- Python packages (automatically installed within the notebook)\n",
        "\n",
        "At the start of the notebook a `data.zip` will be downloaded from a Google Drive and unzipped. This will then provide you with files that contain checkpoints for all of the expensive processing sections such as chunking, generating embeddings and evaluating the pipeline with an LLM as a judge. This saves you money and a lot of time.\n",
        "\n",
        "If you can't or don't want to run this notebook you can also view the completed notebook by opening the `cleantech_rag.html` file in your browser.\n",
        "\n",
        "### Unveiling the Depths of RAG Pipelines\n",
        "\n",
        "Throughout this notebook, we'll delve into the intricate workings of RAG pipelines. Prepare to explore:\n",
        "\n",
        "- Building a Robust RAG Pipeline:\n",
        "  - LLM: GPT-4 turbo\n",
        "  - Semantic Search: ChromaDB with HNSW for fast retrieval\n",
        "  - Embedding Models: OpenAI, BGE-M3, all-MiniLM-L6-v2\n",
        "  - Chunking Strategies: Recursive (256 & 1024 chunk sizes) and Semantic Chunking\n",
        "- Understanding RAG's Inner Workings:\n",
        "  - Examining the embedding space of news articles\n",
        "  - Demystifying the semantic search process\n",
        "- Visualization and Explanation:\n",
        "  - Visualizing the impact of advanced strategies like HyDE and Multi-querying\n",
        "- Evaluation and Comparison:\n",
        "  - RAGAS for LLM-based evaluation\n",
        "  - Traditional retrieval metrics\n",
        "\n",
        "Questions or Issues? We're Here to Help!\n",
        "\n",
        "If you encounter any roadblocks or have questions, please don't hesitate to reach out to [George Rowlands](https://www.linkedin.com/in/georgerowlands/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpXs2xu8gUti",
        "outputId": "fbb2ed2c-9cca-41da-d6bf-660d9f511760"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "chromadb==0.5.0\n",
        "datasets==2.19.1\n",
        "gdown==5.2.0\n",
        "kaggle==1.6.1\n",
        "langchain==0.2.0\n",
        "langchain-community==0.2.0\n",
        "langchain-experimental==0.0.59\n",
        "langchain-openai==0.1.7\n",
        "langdetect==1.0.9\n",
        "lorem-text==2.1\n",
        "nbformat>=4.2.0\n",
        "plotly==5.22.0\n",
        "pretty-jupyter==1.0\n",
        "ragas==0.1.8\n",
        "seaborn==0.13.2\n",
        "sentence-transformers==3.0.0\n",
        "spacy>=3.7\n",
        "textstat==0.7.3\n",
        "umap-learn==0.5.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch==2.3.0 --quiet --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW1BDb7ifakE",
        "outputId": "83870e87-9f59-423d-e178-be9680cdc056"
      },
      "outputs": [],
      "source": [
        "%pip install -r ./requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting your OpenAI Key\n",
        "\n",
        "This OpenAI Key is used for the following tasks:\n",
        "- Generating embeddings for our semantic search.\n",
        "- Leveraging GPT-4-turbo as our LLM in our RAG pipeline and as our judge in RAGAS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile .env\n",
        "\n",
        "OPENAI_API_KEY=WRITE IT HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehyUkxomfakF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import warnings\n",
        "import zipfile\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import chromadb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from chromadb import Collection, Documents, EmbeddingFunction, Embeddings\n",
        "from datasets import Dataset\n",
        "from dotenv import load_dotenv\n",
        "from langdetect import detect\n",
        "from lorem_text import lorem\n",
        "from ragas import RunConfig, evaluate\n",
        "from ragas.metrics import (faithfulness, answer_relevancy, context_relevancy, answer_correctness)\n",
        "from spacy.lang.en import English\n",
        "from textstat import flesch_reading_ease\n",
        "from tqdm import tqdm\n",
        "import umap\n",
        "\n",
        "from langchain.chains.base import Chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma, VectorStore\n",
        "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_core.language_models import LLM\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "load_dotenv()\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gdown 1MoT_s_Zk4dzRRy7E7Va5ZuTROIOI1FfZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"data.zip\", \"r\") as zip_file:\n",
        "    zip_file.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up our LLM\n",
        "\n",
        "To make sure our OpenAI Key is working we will test it by generating a response from GPT-4-turbo which we will later on also be using in our RAG pipeline. Try some different prompts or questions to see how the model responds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
        "question_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Answer the following question: {question}\")\n",
        "question_chain = question_prompt | llm | StrOutputParser()\n",
        "question_chain.invoke({\"question\": \"What is the meaning of life?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAMqEwTDikjI"
      },
      "source": [
        "## Downloading the Dataset from Kaggle\n",
        "\n",
        "We will be exploring the following [Cleantech Media Dataset](https://www.kaggle.com/datasets/jannalipenkova/cleantech-media-dataset). If you have opened this notebook as recommended by opening the provided Github repository in Google Colab then you don't need to to download the dataset. It should already be under `data/bronze`. If not then you can either manually download it and upload it into a `data/bronze` folder or follow the steps below.\n",
        "\n",
        "### Using the Kaggle API\n",
        "\n",
        "We will be using the [Kaggle API](https://github.com/Kaggle/kaggle-api) to download the data.\n",
        "\n",
        "To use the Kaggle API you will need a Kaggle account. If you don't already have one, sign up for a Kaggle account at https://www.kaggle.com. When you are logged in, go to the 'Settings' tab of your user profile `https://www.kaggle.com/settings` and select 'Create New Token'. This will trigger the download of `kaggle.json`, a file containing your API credentials.\n",
        "\n",
        "You can then add your Kaggle username and key from the `kaggle.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_folder = Path(\"./data\")\n",
        "if not data_folder.exists():\n",
        "    data_folder.mkdir()\n",
        "bronze_folder = data_folder / \"bronze\"\n",
        "if not bronze_folder.exists():\n",
        "    bronze_folder.mkdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85WahTMQjQ0u",
        "outputId": "a6d1e870-3b29-4d5f-e99f-d36ebbb03bde"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "kaggle_user = \"XXXXXXXXXXXXXXXX\"\n",
        "kaggle_key = \"XXXXXXXXXXXXXXXX\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R82LIscJjpif",
        "outputId": "688d08d9-69e8-40af-ceb2-e13ae7b7cc1a"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "os.system(f\"kaggle datasets download -d jannalipenkova/cleantech-media-dataset -p {bronze_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfzj0Fxrk9Sk"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "with zipfile.ZipFile(bronze_folder / \"cleantech-media-dataset.zip\", \"r\") as zip_file:\n",
        "    zip_file.extractall(bronze_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUbhV2HofakG"
      },
      "source": [
        "## Loading the Dataset into Dataframes\n",
        "\n",
        "We now load and inspect both the Cleantech Media Dataset and the gold-standard evaluation data provided by our subject matter expert, Janna Lipenkova."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "sMFTpSUffakG",
        "outputId": "6bfdc4da-20aa-4699-e466-920d30c5c4f7"
      },
      "outputs": [],
      "source": [
        "articles_df = pd.read_csv(\n",
        "    bronze_folder / \"cleantech_media_dataset_v2_2024-02-23.csv\",\n",
        "    encoding='utf-8', index_col=0)\n",
        "articles_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "2A72qCg-fakG",
        "outputId": "e1435979-b4f1-4af2-fea8-c1e191b51e09"
      },
      "outputs": [],
      "source": [
        "human_eval_df = pd.read_csv(\n",
        "    bronze_folder / \"cleantech_rag_evaluation_data_2024-02-23.csv\",\n",
        "    encoding='utf-8', index_col=0)\n",
        "human_eval_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD8FZt9Frkov"
      },
      "source": [
        "## Explorative Data Analysis & Preprocessing\n",
        "\n",
        "As the saying goes, \"garbage in, garbage out.\"  In the realm of machine learning, the quality of our outputs hinges on the quality of our inputs.  This section delves into the essential processes of Exploratory Data Analysis (EDA) and data preprocessing. Through EDA, we'll illuminate the characteristics, patterns, and potential quirks residing within our cleantech news article dataset. Preprocessing will ensure our data is cleansed, structured, and prepared to be effectively utilized by the RAG pipeline, laying the foundation for high-quality results.\n",
        "\n",
        "Let us start by gaining an overview of the datasets features (columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "MIh592YOryGs",
        "outputId": "d067cffc-7b57-45d0-d3d4-47cb9a7f3085"
      },
      "outputs": [],
      "source": [
        "articles_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObheDL_Hr0xE",
        "outputId": "01b38f20-6485-4039-c3e5-d320232db9b6"
      },
      "outputs": [],
      "source": [
        "articles_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuxW-_-_r9ad"
      },
      "source": [
        "Our initial exploration reveals that the \"author\" column only contains data for 31 out of 9593 articles. Since this offers minimal information gain, we can remove this feature.\n",
        "\n",
        "We've also observed that some titles and content entries appear to be non-unique. This might necessitate identifying and removing duplicate entries.\n",
        "\n",
        "On a positive note, the article URLs are all unique, potentially serving as suitable unique identifiers for the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df = articles_df.drop(columns=[\"author\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2CPkZUc5W6k"
      },
      "source": [
        "### Article Domains\n",
        "\n",
        "The dataset helpfully provides the domain names extracted from the article URLs. These domains essentially represent the publishers of the news articles. Let's analyze the distribution of publishers and see how many articles each publisher has contributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "domain_counts = articles_df[\"domain\"].value_counts()\n",
        "domain_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A visualization helps us to understand the skew in the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "cxBPYl2qsvea",
        "outputId": "021fa360-6ef6-4921-ed66-753a48ef0656"
      },
      "outputs": [],
      "source": [
        "barplot = sns.barplot(\n",
        "    x=domain_counts.values, \n",
        "    y=domain_counts.index,\n",
        "    hue=domain_counts.index\n",
        ")\n",
        "\n",
        "barplot.set_title('Article Counts by Domain')\n",
        "barplot.set_xlabel('Article Count')\n",
        "barplot.set_ylabel('Domain')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our exploration of article domains reveals a skewed distribution.  Publishers like cleantechnica have a significantly higher representation (1861 articles), while others like indoenergy have minimal contributions (2 articles).  If we proceed with sampling this data, this imbalance should be taken into account. [Stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) is the recommended  approach to ensure a representative sample across different publishers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Article Dates\n",
        "\n",
        "Each article within the dataset is accompanied by a publication date. Let's delve into the temporal range of these articles and investigate any noteworthy patterns in publication trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the amount of articles over time\n",
        "articles_df[\"date\"] = pd.to_datetime(articles_df[\"date\"])\n",
        "time_df = articles_df.groupby(\"date\").size().reset_index()\n",
        "time_df.columns = [\"date\",\"count\"]\n",
        "\n",
        "time_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.lineplot(data=time_df, x=\"date\", y=\"count\")\n",
        "plt.title(\"Article Count Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel(\"Article Count\")\n",
        "# add a line for the average\n",
        "avg_count = time_df[\"count\"].mean()\n",
        "plt.axhline(avg_count, color='r', linestyle='--', label=f\"Average article count per day: {avg_count:.2f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the daily article count appears consistent overall, a significant outlier disrupts the pattern on the 2023-12-05.  The cause of this outlier is undetermined, but it could potentially be the date the data was scraped and the default value assigned for missing dates. Since the publication date is not crucial for RAG pipeline, we can remove it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df = articles_df.drop(columns=[\"date\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Article Titles\n",
        "\n",
        "As noted in our initial exploration, some articles share identical titles. Here, we'll focus on identifying and handling these duplicate titles to ensure a clean and consistent dataset for our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.histplot(articles_df[\"title\"].str.len())\n",
        "plt.title(\"Title Length Distribution\")\n",
        "plt.xlabel(\"Title Length\")\n",
        "plt.ylabel(\"Count\")\n",
        "avg_count = articles_df[\"title\"].str.len().mean()\n",
        "plt.axvline(avg_count, color='r', linestyle='--', label=f\"Average title length: {avg_count:.2f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df[\"title\"].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicate_titles = articles_df[articles_df[\"title\"].duplicated(keep=False)].sort_values(\"title\")\n",
        "duplicate_titles.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicate_titles[\"content\"].duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our exploration identified 24 titles that appear multiple times in the dataset. Examples include \"About David J. Cross.\" Interestingly, while the titles are identical, the content itself appears to be unique.\n",
        "\n",
        "Here are some additional observations for further investigation:\n",
        "- A pattern was found where some articles begin with the phrase \"By clicking.\" We'll delve into this further to determine the potential impact when analzing the article contents.\n",
        "- We can observe instances of articles with seemingly similar content but differing URLs containing \"sgvoice.energyvoice.com\" and \"energyvoice.com.\" Let's explore these cases to understand the potential distinction between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wrap_text(text: str, char_per_line=100) -> str:\n",
        "    # for better readability, wrap the text at the last space before the char_per_line\n",
        "    if len(text) < char_per_line:\n",
        "        return text\n",
        "    else:\n",
        "        return text[:char_per_line].rsplit(' ', 1)[0] + '\\n' + wrap_text(text[len(text[:char_per_line].rsplit(' ', 1)[0])+1:], char_per_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(duplicate_titles.iloc[0][\"title\"])\n",
        "print(wrap_text(duplicate_titles.iloc[0][\"content\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(duplicate_titles.iloc[1][\"title\"])\n",
        "print(wrap_text(duplicate_titles.iloc[1][\"content\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our analysis suggests potential redundancy within certain articles. In some cases, the second article might appear to be the first article with an additional sentence appended at the end.\n",
        "\n",
        "Let's take a closer look at these \"energyvoice\" articles and how the contents start and see if we can eliminate these redundancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "energyvoice_articles = articles_df[articles_df[\"domain\"].str.contains(\"energyvoice\")]\n",
        "energyvoice_articles.content.map(lambda x: x[:50]).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_prefix_articles(df: pd.DataFrame, prefix_len: int = 100) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Takes O(n^2) time complexity\n",
        "    If the first {prefix_len} characters of the article are the same, then we consider them as a prefix. \n",
        "    If an article is a prefix of a longer article, then we remove it.\n",
        "    If an article is a prefix of longer article, but they have different titles, then we keep them.\n",
        "    \"\"\"\n",
        "\n",
        "    df[\"char_len\"] = df[\"content\"].map(len)\n",
        "    df = df.sort_values(by='char_len', ascending=True).reset_index(drop=True)\n",
        "\n",
        "    # Initialize a list to keep the articles that are not prefixes of others\n",
        "    non_prefix_articles = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        is_prefix = False\n",
        "        content_i = row['content'][:prefix_len]\n",
        "        title_i = row['title']\n",
        "\n",
        "        for j in range(i + 1, len(df)):\n",
        "            content_j = df.at[j, 'content'][:prefix_len]\n",
        "            title_j = df.at[j, 'title']\n",
        "\n",
        "            if content_i == content_j:\n",
        "                # If the prefix matches but the titles are different, we keep it\n",
        "                if title_i != title_j:\n",
        "                    continue\n",
        "                else:\n",
        "                    is_prefix = True\n",
        "                    break\n",
        "\n",
        "        if not is_prefix:\n",
        "            non_prefix_articles.append(row)\n",
        "\n",
        "    print(f\"Removed {len(df) - len(non_prefix_articles)} prefix articles\")\n",
        "    return pd.DataFrame(non_prefix_articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "energyvoice_articles = remove_prefix_articles(energyvoice_articles)\n",
        "energyvoice_articles.content.map(lambda x: x[:100]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There still seem to be be some redundancy, but we did manage to remove 11 duplicates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKth2wAWsscv"
      },
      "source": [
        "### Article Contents\n",
        "\n",
        "Having explored various aspects of our dataset, we now turn our attention to the heart of the matter: the article content itself. This section will delve into the analysis and preprocessing techniques we'll employ to ensure the content is high-quality and effectively utilized by our RAG pipeline.\n",
        "\n",
        "We start with a visual inspection of the article content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYZcHix2fakG"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7)\n",
        "random_sample_id = np.random.choice(articles_df.index)\n",
        "print(wrap_text(articles_df.loc[random_sample_id, \"content\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY06fPZpfakH"
      },
      "source": [
        "Our initial examination reveals that article content is currently stored as a list of strings. To gain deeper understanding and facilitate preprocessing, we'll transform these lists into a more cohesive textual format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-YQ-PpLfakH"
      },
      "outputs": [],
      "source": [
        "articles_df['article'] = articles_df['content'].apply(lambda x: ' '.join(eval(x)))\n",
        "print(wrap_text(articles_df.loc[random_sample_id, \"article\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df[\"article\"].duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicate_articles = articles_df[articles_df[\"article\"].duplicated(keep=False)].sort_values(\"article\")\n",
        "duplicate_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our analysis uncovers additional insights regarding content duplication. We observe cases where seemingly identical articles are reposted on the same domain but with different titles (excluding the \"sgvoice.energyvoice.com\" vs. \"energyvoice.com\" scenario previously addressed). Here, we'll strategically keep these duplicates where contents are the same but titles are different.\n",
        "\n",
        "**Importance of Titles**\n",
        "\n",
        "We keep these duplicate articles because titles can hold information relevant for our RAG pipeline. Consider a scenario where a user query uses an abbreviation, while the corresponding article only contains the abbreviation in the title, in the content always the full term is used. To bridge this gap, we'll prepend titles to the article content during preprocessing. This ensures that the retrieval process considers not only the content itself, but also the potentially informative titles.\n",
        "\n",
        "**Next Step**\n",
        "\n",
        "As previously noted, some articles exhibit standardized introductions, possibly artifacts of the data scraping process. We'll develop appropriate techniques to handle these introductions during preprocessing, ensuring they don't hinder the effectiveness of our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFS7HGF4fakH"
      },
      "outputs": [],
      "source": [
        "articles_df.article.map(lambda x: x[:50]).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHbMZXyUfakH"
      },
      "outputs": [],
      "source": [
        "artifacts = [\n",
        "    \"By clicking `` Allow All '' you agree to the sto\",\n",
        "    \"Sign in to get the best natural gas news and dat\",\n",
        "    \"window.dojoRequire ( [ `` mojo/signup-forms/Load\"\n",
        "]\n",
        "\n",
        "for artifact in artifacts:\n",
        "    print(wrap_text(articles_df[articles_df.article.str.startswith(artifact)].article.iloc[0][:500]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_scrapping_artifacts(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
        "    text_artifacts = [\n",
        "        \"By clicking `` Allow All '' you agree to the storing of cookies on your device to enhance site navigation, analyse site usage and support us in providing free open access scientific content. More info.\",\n",
        "        \"Sign in to get the best natural gas news and data. Follow the topics you want and receive the daily emails. Your email address * Your password * Remember me Continue Reset password Featured Content News & Data Services Client Support\"\n",
        "    ]\n",
        "\n",
        "    regex_artifacts = [\n",
        "        r\"window.dojoRequire \\( \\[ .*\\}\\) \\}\\) \"\n",
        "    ]\n",
        "\n",
        "    for pattern in text_artifacts:\n",
        "        articles_df[column] = articles_df[column].str.replace(pattern, '', regex=False)\n",
        "\n",
        "    for pattern in regex_artifacts:\n",
        "        articles_df[column] = articles_df[column].str.replace(pattern, '', regex=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df = remove_scrapping_artifacts(articles_df, \"article\")\n",
        "articles_df.article.map(lambda x: x[:50]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0vvHoPZfakH"
      },
      "source": [
        "Our efforts have successfully eliminated a substantial portion of the scrapping artifacts within the articles. However, some traces  still persist, likely remnants of past website navigation structures. While removing these remaining artifacts could offer further refinement, it also presents a significant challenge. Therefore, we'll acknowledge this for now and move onto further preprocessing such as filtering out articles that are not written in English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df[\"lang\"] = articles_df[\"article\"].map(detect)\n",
        "articles_df[\"lang\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will first inspect the language-specific assessment of our texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df[articles_df[\"lang\"] != \"en\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(wrap_text(articles_df[articles_df[\"lang\"] != \"en\"].iloc[1][\"article\"][1000:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df = articles_df[articles_df[\"lang\"] == \"en\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our exploration revealed a small number of articles containing non-English content (some in German and 1 with a Russian section).  Since most LLMs and embedding models are primarily trained on English text, removing these articles ensures compatibility with our chosen models for this notebook. For simplicity, we'll only focus on supporting English queries and responses within this RAG pipeline.\n",
        "\n",
        "#### Challenges of Multilingual RAG Pipelines\n",
        "\n",
        "Introducing multilingual capabilities into a RAG pipeline presents an additional layer of complexity. Here's a breakdown of some key challenges:\n",
        "\n",
        "- **Multilingual Model Support:** Both the LLM and embedding models need to be proficient in all target languages (e.g., English and German). The LLM must be able to comprehend and generate text in these languages, while the embedding models should effectively map similar concepts across languages into the same semantic space.\n",
        "- **Prompt Engineering for Multilingual Responses:** When a user submits a question in German, for instance, we'd ideally retrieve relevant articles, potentially also in English which can distract the LLM, and utilize prompt engineering to ensure the LLM generates a response in German.\n",
        "\n",
        "#### Characters, Tokens and Words\n",
        "\n",
        "Let us further analyze the contents of the articles. However, before we do so let us define the meaning of characters, tokens and words:\n",
        "- **Characters:** The smallest unit of text, including letters, numbers, punctuation, and whitespace.\n",
        "- **Tokens:** Most NLP models operate on tokens, which are sequences of characters that represent a semantic unit. These units can be words, subwords, or characters. Tokenization is the process of converting text into tokens. To see the tokenization process in action for the OpenAI GPT-4 model, check out the [OpenAI GPT-4 Tokenizer](https://platform.openai.com/tokenizer).\n",
        "- **Words:** Just as in everyday's language, words are the building blocks of text. They are composed of one or more characters and are separated by whitespace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gc2TNbFfakH"
      },
      "outputs": [],
      "source": [
        "sns.histplot(articles_df[\"article\"].map(len), kde=True)\n",
        "\n",
        "plt.title(\"Amount of characters in articles\")\n",
        "plt.xlabel(\"Amount of characters\")\n",
        "plt.ylabel(\"Number of articles\")\n",
        "median_char_len = articles_df[\"article\"].map(len).median()\n",
        "mean_char_len = articles_df[\"article\"].map(len).mean()\n",
        "plt.axvline(median_char_len, color='r', linestyle='--', label=f\"Median character amount: {median_char_len:.2f}\")\n",
        "plt.axvline(mean_char_len, color='g', linestyle='--', label=f\"Mean character amount: {mean_char_len:.2f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.histplot(articles_df[\"article\"].map(lambda x: len(x.split())), kde=True)\n",
        "\n",
        "plt.title(\"Amount of words in articles\")\n",
        "plt.xlabel(\"Amount of words\")\n",
        "plt.ylabel(\"Number of articles\")\n",
        "median_word_len = articles_df[\"article\"].map(lambda x: len(x.split())).median()\n",
        "mean_word_len = articles_df[\"article\"].map(lambda x: len(x.split())).mean()\n",
        "plt.axvline(median_word_len, color='r', linestyle='--', label=f\"Median word amount: {median_word_len:.2f}\")\n",
        "plt.axvline(mean_word_len, color='g', linestyle='--', label=f\"Mean word amount: {mean_word_len:.2f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nlp = English()\n",
        "tokenizer = nlp.tokenizer\n",
        "\n",
        "sns.histplot(articles_df[\"article\"].map(lambda x: len(tokenizer(x))), kde=True)\n",
        "\n",
        "plt.title(\"Amount of tokens in articles\")\n",
        "plt.xlabel(\"Amount of tokens\")\n",
        "plt.ylabel(\"Number of articles\")\n",
        "median_token_len = articles_df[\"article\"].map(lambda x: len(tokenizer(x))).median()\n",
        "mean_token_len = articles_df[\"article\"].map(lambda x: len(tokenizer(x))).mean()\n",
        "plt.axvline(median_token_len, color='r', linestyle='--', label=f\"Median token amount: {median_token_len:.2f}\")\n",
        "plt.axvline(mean_token_len, color='g', linestyle='--', label=f\"Mean token amount: {mean_token_len:.2f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_tokens = [token.text for article in articles_df[\"article\"] for token in tokenizer(article)]\n",
        "# remove non-alphabetic tokens such as punctuation\n",
        "alpha_tokens = [token for token in all_tokens if token.isalpha()]\n",
        "alpha_tokens = [token.lower() for token in alpha_tokens]\n",
        "alpha_token_counts = Counter(alpha_tokens)\n",
        "\n",
        "sns.barplot(\n",
        "    x=[count for token, count in alpha_token_counts.most_common(20)],\n",
        "    y=[token for token, count in alpha_token_counts.most_common(20)],\n",
        "    hue=[token for token, count in alpha_token_counts.most_common(20)]\n",
        ")\n",
        "\n",
        "plt.title(\"Most common alphabetic tokens\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Token\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The initial approach returns common words which do not reflect the subject-specific nature of our document collection. We will remove them to understand the content of the texts better. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove stopwords such as 'the', 'a', 'and'\n",
        "non_stop_tokens = [token for token in alpha_tokens if not nlp.vocab[token].is_stop]\n",
        "non_stop_token_counts = Counter(non_stop_tokens)\n",
        "\n",
        "sns.barplot(\n",
        "    x=[count for token, count in non_stop_token_counts.most_common(20)],\n",
        "    y=[token for token, count in non_stop_token_counts.most_common(20)],\n",
        "    hue=[token for token, count in non_stop_token_counts.most_common(20)]\n",
        ")\n",
        "\n",
        "plt.title(\"Most common non-stopword tokens\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Token\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As one would expect in a dataset of cleantech news articles most of the tokens that are not punctation or stopwords revolve around the subjects of energy, climate, and technology. This is a good sign that the dataset is relevant to the topic at hand. The \"s\" token comes up frequently, which is likely due to the possessive form of words. With an average of around 700 words per article, we can expect a good amount of information to be present in each article and an average reading time of around 3-4 minutes.\n",
        "\n",
        "#### Flesch Reading Ease Score\n",
        "\n",
        "The Flesch Reading Ease Score (FRES, a.k.a Flesch-Kincaid Reading Ease Score) is a heuristic used to evaluate how easy it is to understand a text based on the length of sentences and the number of syllables per word. Scores can range from -100 (very difficult to read) to 100 (very easy to read). Scores below 50 are indicative of difficult texts for College level. This metric can be useful for assessing the readability of our articles and ensuring they are accessible to a broad audience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "articles_df[\"readability\"] = articles_df[\"article\"].apply(flesch_reading_ease)\n",
        "\n",
        "sns.histplot(articles_df[\"readability\"], kde=True)\n",
        "\n",
        "plt.title(\"Flesch Reading Ease of articles\")\n",
        "plt.xlabel(\"Flesch Reading Ease\")\n",
        "plt.ylabel(\"Number of articles\")\n",
        "mean_readability = articles_df[\"readability\"].mean()\n",
        "plt.axvline(mean_readability, color='g', linestyle='--', label=f\"Mean readability: {mean_readability:.2f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We analyze now the diversity of language complexity used by different publishing domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "domains = articles_df[\"domain\"].unique()\n",
        "\n",
        "# Setup the subplots based on the number of domains\n",
        "plots_per_row = 3\n",
        "num_rows = (len(domains) + 2) // plots_per_row \n",
        "plot_height = 6 \n",
        "fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(plot_height * plots_per_row, plot_height * num_rows))\n",
        "axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
        "\n",
        "# Plot for each domain\n",
        "for i, domain in enumerate(domains):\n",
        "    domain_articles = articles_df[articles_df[\"domain\"] == domain]\n",
        "    sns.histplot(domain_articles[\"readability\"], kde=True, ax=axes[i], bins=30)\n",
        "    axes[i].set_title(f'Readability of {domain}')\n",
        "    axes[i].set_xlabel('Flesch Reading Ease Score')\n",
        "    axes[i].set_ylabel(\"Number of articles\")\n",
        "    mean_readability = domain_articles[\"readability\"].mean()\n",
        "    axes[i].axvline(mean_readability, color='g', linestyle='--', label=f\"Mean readability: {mean_readability:.2f}\")\n",
        "\n",
        "# remove the empty plots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To gauge the readability of our articles, we calculated the Flesch Reading Ease Score. The average score of around 45 indicates a \"fairly easy\" reading level, which is positive news. This suggests the content is likely accessible to a broad audience and, consequently, understandable by our RAG pipeline as well.\n",
        "\n",
        "Our analysis revealed a consistent average Flesch Reading Ease Score across most of the identified domains, with minor variations. This indicates a relatively consistent level of readability across different publishers within the dataset.\n",
        "\n",
        "Finally we will save the cleaned dataset to a new file in the `data/silver` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "silver_folder = data_folder / \"silver\"\n",
        "if not silver_folder.exists():\n",
        "    silver_folder.mkdir()\n",
        "\n",
        "articles_df.to_csv(silver_folder / \"articles.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation Data\n",
        "\n",
        "Next we will analyze the provided evaluation data and ensure that they match the content of the articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "human_eval_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "human_eval_df.rename(columns={\"relevant_chunk\":\"relevant_section\",\"article_url\": \"url\"}, inplace=True)\n",
        "human_eval_df.drop(columns=[\"question_id\"], inplace=True)\n",
        "human_eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.histplot(human_eval_df[\"question\"].map(len), kde=True)\n",
        "plt.title(\"Question Character Length Distribution\")\n",
        "plt.xlabel(\"Character Length\")\n",
        "plt.ylabel(\"Count\")\n",
        "mean_char_len = human_eval_df[\"question\"].map(len).mean()\n",
        "plt.axvline(mean_char_len, color='r', linestyle='--', label=f\"Mean character amount: {mean_char_len:.2f}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_articles = human_eval_df.copy()\n",
        "missing_articles = missing_articles[~human_eval_df[\"url\"].isin(articles_df[\"url\"])]\n",
        "missing_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our exploration has identified instances where articles linked to specific questions appear to be missing from the dataset.  To determine the root cause, let's investigate whether these articles are genuinely absent or if inconsistencies in URL formatting are creating the illusion of missing data.  Normalizing the URLs across the dataset will help us differentiate between these two scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_url(url: str) -> str:\n",
        "    url = url.replace(\"https://\", \"\")\n",
        "    url = url.replace(\"http://\", \"\")\n",
        "    url = url.replace(\"www.\", \"\")\n",
        "    url = url.rstrip(\"/\")\n",
        "    return url\n",
        "\n",
        "articles_df[\"url\"] = articles_df[\"url\"].map(normalize_url)\n",
        "human_eval_df[\"url\"] = human_eval_df[\"url\"].map(normalize_url)\n",
        "\n",
        "missing_articles = human_eval_df.copy()\n",
        "missing_articles = missing_articles[~human_eval_df[\"url\"].isin(articles_df[\"url\"])]\n",
        "missing_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also know from previous analysis that some duplicate articles from the \"energyvoice\" domain so we will also normalize these URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_articles[\"url\"] = missing_articles[\"url\"].map(lambda x: x.replace(\"sgvoice.net\", \"sgvoice.energyvoice.com\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_articles[~missing_articles[\"url\"].isin(articles_df[\"url\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "human_eval_df.loc[missing_articles.index, \"url\"] = missing_articles[\"url\"]\n",
        "human_eval_df[human_eval_df[\"url\"].isin(articles_df[\"url\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the end we are able to find all the articles that are linked to the evaluation data and have therefore successfully completed our exploratory data analysis and preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s5o91NcfakI"
      },
      "source": [
        "## Subsampling\n",
        "\n",
        "For faster processing and to reduce the cost of running the notebook we will subsample the dataset to 1000 articles. This will allow us to run the notebook in a reasonable amount of time and still provide meaningful results. Because the distribution of articles across publishers is skewed we will use stratified sampling to ensure that we have a representative sample. We also need to keep in mind that the evaluation data are linked to specific articles so we need to make sure that these are included in the subsample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLXlib8ZfakI"
      },
      "outputs": [],
      "source": [
        "eval_articles_df = articles_df[articles_df[\"url\"].isin(human_eval_df[\"url\"])]\n",
        "eval_articles_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Hg4LGafakI"
      },
      "outputs": [],
      "source": [
        "print(eval_articles_df[\"url\"].unique().shape)\n",
        "print(human_eval_df[\"url\"].unique().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b63qmjwfakM"
      },
      "outputs": [],
      "source": [
        "def do_stratification(\n",
        "        df: pd.DataFrame,\n",
        "        column: str,\n",
        "        sample_size: int,\n",
        "        seed: int = 42\n",
        ") -> pd.DataFrame:\n",
        "    res_df = df.copy()\n",
        "    indx = df.groupby(column, group_keys=False)[column].apply(lambda x: x.sample(n=int(sample_size/len(df) * len(x)), random_state=seed)).index.to_list()\n",
        "    return res_df.loc[indx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_df = do_stratification(articles_df, \"domain\", 1000, 69)\n",
        "# if the articles are already in the subsample from the evaluation set, then we remove them, so we just want unique urls\n",
        "sample_df = sample_df[~sample_df[\"url\"].isin(eval_articles_df[\"url\"])]\n",
        "sample_df = pd.concat([sample_df, eval_articles_df])\n",
        "sample_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make sure that the distributional characteristics has not been changed by subsampling we visualize and compare both data sets in relative terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kWzSNWEfakM"
      },
      "outputs": [],
      "source": [
        "original_domain_counts = articles_df[\"domain\"].value_counts().to_frame()\n",
        "original_domain_counts = original_domain_counts / original_domain_counts.sum() * 100\n",
        "domain_counts_df = original_domain_counts.copy()\n",
        "domain_counts_df[\"type\"] = \"Original\"\n",
        "\n",
        "\n",
        "sample_domain_counts = sample_df[\"domain\"].value_counts().to_frame()\n",
        "sample_domain_counts = sample_domain_counts / sample_domain_counts.sum() * 100\n",
        "sample_domain_counts[\"type\"] = \"Sample\"\n",
        "\n",
        "domain_counts_df = pd.concat([domain_counts_df, sample_domain_counts])\n",
        "sns.barplot(\n",
        "    x=domain_counts_df.index,\n",
        "    y=domain_counts_df[\"count\"],\n",
        "    hue=domain_counts_df[\"type\"]\n",
        ")\n",
        "plt.title(\"Domain Distribution\")\n",
        "plt.xlabel(\"Domain\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now all is prepared to start developing our RAG! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chunking\n",
        "\n",
        "Chunking is a crucial step in the RAG pipeline. It involves breaking down the articles into smaller, more manageable pieces. \n",
        "\n",
        "![chunking](https://cohere.com/_ipx/w_3840,q_75/https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Frag-chatbot-embedding.png?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Frag-chatbot-embedding.png&w=3840&q=75)\n",
        "\n",
        "There are mainly two reasons for this:\n",
        "- **Generation:** The LLM has a limit on the number of tokens it can process at once. By chunking the articles, we can ensure that the LLM can generate responses without running into this limit. Another reason to use chunks for the generation step is to avoid \"distractions\" from irrelevant parts of the article. Just like if you were given a book and asked to answer a question about a the book it would be easier if you were just given the relevant chapter.\n",
        "- **Retrieval:** Just like the LLM, the embedding model in the retrieval step has a limit on the number of tokens it can process at once. By chunking the articles, we can ensure that the embedding model can process the entire article. By chunking the articles, we can also improve the retrieval performance by having fine-grained chunks that can be matched more closely to the user query, rather then more general chunks.\n",
        "\n",
        "Let's start by getting a better feeling for the most common size of chunks based on the number of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lorem_text(num_chars: int) -> str:\n",
        "    expected_avg_word_len = 3 # on the lower side to be safe\n",
        "    text = lorem.words(num_chars // expected_avg_word_len)\n",
        "    return text[:num_chars]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(wrap_text(get_lorem_text(256)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(wrap_text(get_lorem_text(512)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(wrap_text(get_lorem_text(1024)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(wrap_text(get_lorem_text(2048)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the Chunks\n",
        "\n",
        "In this notebook we will be using two different chunking strategies:\n",
        "- **Recursive Chunking:** This strategy involves recursively splitting the article into smaller chunks based on the article structure such as paragraphs and sentences until the chunk size is less than or equal to the maximum chunk size.\n",
        "- **Semantic Chunking:** This strategy involves splitting the article into chunks based on semantic boundaries. This strategy finds boundaries between sentences that are semantically different and splits the article at these boundaries to create chunks. To do this we will need to use an embedding model to calculate the similarity between sentences. These embedding models will then also be used in the retrieval step to find the most relevant chunks.\n",
        "\n",
        "To see how different texts get chunked with different strategies and chunk sizes check out the [Chunking Visualizer](https://chunkviz.up.railway.app/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_recursive_splitter(chunk_size: int, chunk_overlap: int) -> TextSplitter:\n",
        "    return RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
        "        length_function=len,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the recursive splitter mainly relies on newlines, are there even any? No, so it will focus on sentences.\n",
        "sample_df[\"article\"].map(lambda x: x.count(\"\\n\")).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us set the device for efficient use of available resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if we can make use of any device that is better than the CPU, we will use it\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "\n",
        "model_kwargs = {'device': device, \"trust_remote_code\": True}\n",
        "model_kwargs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We select three embedding models from HuggingFace to represent our text fragments in numerical forma in a vector space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_models = {\n",
        "    \"mini\": HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs=model_kwargs),\n",
        "    \"bge-m3\": HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\", model_kwargs=model_kwargs),\n",
        "    \"gte\": HuggingFaceEmbeddings(model_name=\"Alibaba-NLP/gte-base-en-v1.5\", model_kwargs=model_kwargs),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also define the chunking strategies to be used. The recursive splittering is characterized by the length of chunks and the overlap between adjacent chunks. For the semantic chunking, sentences embedded as dense vectors are merged as long as the cosine distance between two consecutive sentences does not exceed a percentile based threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "recursive_256_splitter = get_recursive_splitter(256, 64)\n",
        "recursive_1024_splitter = get_recursive_splitter(1024, 128)\n",
        "semantic_splitter = SemanticChunker(\n",
        "    embedding_models[\"mini\"], breakpoint_threshold_type=\"percentile\"\n",
        ")\n",
        "splitters = {\n",
        "    \"recursive_256\": recursive_256_splitter,\n",
        "    \"recursive_1024\": recursive_1024_splitter,\n",
        "    \"semantic\": semantic_splitter\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CvyzixhfakM"
      },
      "outputs": [],
      "source": [
        "def chunk_documents(df: pd.DataFrame, text_splitter: TextSplitter):\n",
        "    chunks = []\n",
        "    id = 0\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        article_content = row['article']\n",
        "        title = row['title']\n",
        "        # we add the title to the content as it might be relevant to the question\n",
        "        full_text = title + \": \" + article_content\n",
        "        char_chunks = text_splitter.split_text(full_text)\n",
        "        for chunk in char_chunks:\n",
        "                id += 1\n",
        "                # add metadata to the chunk for potential later use\n",
        "                metadata = {\n",
        "                    'title': row['title'],\n",
        "                    'url': row['url'],\n",
        "                    'domain': row['domain'],\n",
        "                    'id': id,\n",
        "                }\n",
        "                chunks.append(Document(\n",
        "                    page_content=chunk,\n",
        "                    metadata=metadata,\n",
        "                ))\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks_folder = silver_folder / \"chunks\"\n",
        "if not chunks_folder.exists():\n",
        "    chunks_folder.mkdir()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function will load existing chunks, prepared for our tutorial to speed up the preparation process.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_or_create_chunks(df: pd.DataFrame, text_splitter: TextSplitter, splitter_name: str) -> List[Document]:\n",
        "    chunks_file = chunks_folder / f\"{splitter_name}_chunks.json\"\n",
        "    if chunks_file.exists():\n",
        "        with open(chunks_file, \"r\") as file:\n",
        "            chunks = [Document(**chunk) for chunk in json.load(file)]\n",
        "        print(f\"Loaded {len(chunks)} chunks from {chunks_file}\")\n",
        "    else:\n",
        "        chunks = chunk_documents(df, text_splitter)\n",
        "        with open(chunks_file, \"w\") as file:\n",
        "            json.dump([doc.dict() for doc in chunks], file, indent=4)\n",
        "        print(f\"Saved {len(chunks)} chunks to {chunks_file}\")\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOXG75fXfakM"
      },
      "outputs": [],
      "source": [
        "chunks = {}\n",
        "for splitter_name, splitter in splitters.items():\n",
        "    chunks[splitter_name] = get_or_create_chunks(sample_df, splitter, splitter_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have created and saved the chunks we can analyze them. We can already see above that the semantic chunks are generally larger than the recursive chunks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing the Chunks\n",
        "\n",
        "Let's start by looking at the first chunk of the first article to get a feeling for what the chunks look like depending on the chunking strategy and then we will look at the distribution of the chunk sizes and the number of chunks per article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for splitter_name, splitter_chunks in chunks.items():\n",
        "    print(f\"{splitter_name} chunks:\")\n",
        "    print(wrap_text(splitter_chunks[0].page_content, char_per_line=150))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_chunk_lengths(chunks: List[Document], title: str):\n",
        "    sns.histplot([len(chunk.page_content) for chunk in chunks], kde=True)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Chunk length\")\n",
        "    plt.ylabel(\"Number of chunks\")\n",
        "    median_chunk_len = np.median([len(chunk.page_content) for chunk in chunks])\n",
        "    mean_chunk_len = np.mean([len(chunk.page_content) for chunk in chunks])\n",
        "    plt.axvline(median_chunk_len, color='r', linestyle='--', label=f\"Median chunk length: {median_chunk_len:.2f}\")\n",
        "    plt.axvline(mean_chunk_len, color='g', linestyle='--', label=f\"Mean chunk length: {mean_chunk_len:.2f}\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_chunk_lengths(chunks[\"recursive_256\"], \"Chunk lengths for recursive 256 splitter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_chunk_lengths(chunks[\"recursive_1024\"], \"Chunk lengths for recursive 1024 splitter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_chunk_lengths(chunks[\"semantic\"], \"Chunk lengths for semantic splitter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks_per_article = {splitter_name: Counter([chunk.metadata[\"title\"] for chunk in chunks]) for splitter_name, chunks in chunks.items()}\n",
        "counts = {splitter_name: [count for title, count in chunk_counts.items()] for splitter_name, chunk_counts in chunks_per_article.items()}\n",
        "\n",
        "sns.histplot(counts, kde=True)\n",
        "plt.title(\"Number of chunks per article\")\n",
        "plt.xlabel(\"Number of chunks\")\n",
        "plt.ylabel(\"Number of articles\")\n",
        "plt.legend(chunks_per_article.keys())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From our analysis of our created chunks we can see that the recursive chunks are all around the same size, close to the defined maximum. On the other hand, the semantic chunks vary in size. This is because the semantic chunking strategy is based on the semantic boundaries of the article. \n",
        "\n",
        "We can also see that despite the semantic chunks being larger, the distribution of the number of chunks per article is much wider for the recursive chunks. This is because the recursive chunks are all around the same size, while the semantic chunks have many smaller ones and a few larger ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA56VelMfakN"
      },
      "source": [
        "## Generating Embeddings\n",
        "\n",
        "Now that we have clean chunks, the next step involves generating embeddings for our article chunks. These embeddings will serve as a crucial component for efficient retrieval within the RAG pipeline. For our vector store we'll utilize ChromaDB, a powerful tool for indexing and searching high-dimensional data. To integrate our chosen embedding models with ChromaDB, we'll define a custom wrapper class. This wrapper class will act as an intermediary, ensuring seamless communication between the models and the ChromaDB indexing system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aImVLO9DfakN"
      },
      "outputs": [],
      "source": [
        "class CustomChromadbEmbeddingFunction(EmbeddingFunction):\n",
        "\n",
        "    def __init__(self, model) -> None:\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def _embed(self, l):\n",
        "        return [self.model.embed_query(x) for x in l]\n",
        "\n",
        "    def embed_query(self, query):\n",
        "        return self._embed([query])\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        embeddings = self._embed(input)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We prepare three different embedding models in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chroma_embedding_functions = {\n",
        "    \"mini\": CustomChromadbEmbeddingFunction(embedding_models[\"mini\"]),\n",
        "    \"bge-m3\": CustomChromadbEmbeddingFunction(embedding_models[\"bge-m3\"]),\n",
        "    \"gte\": CustomChromadbEmbeddingFunction(embedding_models[\"gte\"]),\n",
        "}\n",
        "for name, embedding_function in chroma_embedding_functions.items():\n",
        "    sample = embedding_function([\"Hello, world!\"])[0][:5]\n",
        "    print(f\"{name} embedding sample: {sample}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating embeddings can be a computationally intensive process. To optimize efficiency and avoid redundant computations, we'll leverage checkpointing. This technique involves storing the generated embeddings along with their corresponding article chunks. We'll define a simple class to encapsulate this data, facilitating efficient retrieval and reducing the need for recalculating embeddings unless absolutely necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings_folder = silver_folder / \"embeddings\"\n",
        "if not embeddings_folder.exists():\n",
        "    embeddings_folder.mkdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentEmbedding():\n",
        "    def __init__(self, document: Document, text_embedding: List[float]) -> None:\n",
        "        self.document = document\n",
        "        self.text_embedding = text_embedding\n",
        "    \n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            \"document\": self.document.dict(),\n",
        "            \"text_embedding\": self.text_embedding\n",
        "        }\n",
        "    \n",
        "    @classmethod\n",
        "    def from_dict(cls, d: Dict) -> \"DocumentEmbedding\":\n",
        "        return cls(\n",
        "            document=Document(**d[\"document\"]),\n",
        "            text_embedding=d[\"text_embedding\"]\n",
        "        )\n",
        "\n",
        "\n",
        "def get_or_create_embeddings(\n",
        "        embedding_function: EmbeddingFunction,\n",
        "        chunks: List[Document],\n",
        "        embedding_name: str,\n",
        ") -> List[DocumentEmbedding]:\n",
        "    embeddings_file = embeddings_folder / f\"{embedding_name}_embeddings.json\"\n",
        "    if embeddings_file.exists():\n",
        "        with open(embeddings_file, \"r\") as file:\n",
        "            embeddings = [DocumentEmbedding.from_dict(embedding) for embedding in json.load(file)]\n",
        "        print(f\"Loaded {len(embeddings)} embeddings from {embeddings_file}\")\n",
        "    else:\n",
        "        embeddings = []\n",
        "        for chunk in tqdm(chunks):\n",
        "            text_embedding = embedding_function([chunk.page_content])[0]\n",
        "            embedding = DocumentEmbedding(\n",
        "                document=chunk,\n",
        "                text_embedding=text_embedding\n",
        "            )\n",
        "            embeddings.append(embedding)\n",
        "        with open(embeddings_file, \"w\") as file:\n",
        "            json.dump([embedding.to_dict() for embedding in embeddings], file, indent=4)\n",
        "        print(f\"Saved {len(embeddings)} embeddings to {embeddings_file}\")\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = {}\n",
        "for embedding_name, embedding_function in chroma_embedding_functions.items():\n",
        "    for splitter_name, splitter_chunks in chunks.items():\n",
        "        embeddings[f\"{embedding_name}_{splitter_name}\"] = get_or_create_embeddings(\n",
        "            embedding_function, splitter_chunks, f\"{embedding_name}_{splitter_name}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of embeddings relates to the number of chunks produced by the individual chunking strategies, not the embedding dimensions. Thus smaller chunk size (e.g. 256) yields more chunks than larger chunk size (1024), and semantic embeddings even less chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Storing the Embeddings in ChromaDB\n",
        "\n",
        "As mentioned above for our semantic search retrieval we will be storing the embeddings in ChromaDB. ChromaDB is a powerful tool for indexing and searching high-dimensional data. It is allows e.g. to use approximate nearest neighbor (ANN) search based on the Hierarchical Navigable Small World (HNSW) algorithm, which is known for its efficiency in searching high-dimensional spaces.\n",
        "\n",
        "Just like with normal SQL databases we have a server, in this case an SQLite server, that we can connect to with a client. We will then use the client to connect to the server and create for each set of embeddings a new seperate database which can be thought of as the index or a vector space. ChromaDB calls these separate vector spaces \"collections\". These collections will then be used to search for the most relevant chunks to a user query.\n",
        "\n",
        "![semantic search](https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/bltf137a833984d3581/63728faba32209106e8b0b72/vector-search-diagram.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gold_folder = data_folder / \"gold\"\n",
        "if not gold_folder.exists():\n",
        "    gold_folder.mkdir()\n",
        "chromadb_folder = gold_folder / \"chromadb\"\n",
        "if not chromadb_folder.exists():\n",
        "    chromadb_folder.mkdir()\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=chromadb_folder.as_posix())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we can make use of preprocessed data as before to speed up the preparatory steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6c0g28QfakN"
      },
      "outputs": [],
      "source": [
        "def get_or_create_collection(\n",
        "        name: str,\n",
        "        embedding_function: EmbeddingFunction,\n",
        "        embeddings: List[DocumentEmbedding],\n",
        "        batch_size: int = 128\n",
        ") -> Collection:\n",
        "\n",
        "    collection = chroma_client.get_or_create_collection(\n",
        "        name=name,\n",
        "        # configure to use cosine distance not default L2\n",
        "        metadata={\"hnsw:space\": \"cosine\"},\n",
        "        embedding_function=embedding_function\n",
        "    )\n",
        "\n",
        "    if collection.count() == 0:\n",
        "        for i in tqdm(range(0, len(embeddings), batch_size)):\n",
        "            batch = embeddings[i:i+batch_size]\n",
        "            collection.add(\n",
        "                documents=[embedding.document.page_content for embedding in batch],\n",
        "                embeddings=[embedding.text_embedding for embedding in batch],\n",
        "                ids=[str(embedding.document.metadata[\"id\"]) for embedding in batch],\n",
        "                metadatas=[embedding.document.metadata for embedding in batch]\n",
        "            )\n",
        "\n",
        "    return collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w3U5NmYfakN"
      },
      "outputs": [],
      "source": [
        "collections = {}\n",
        "for collection_name, current_embeddings in embeddings.items():\n",
        "    collection = get_or_create_collection(\n",
        "        collection_name,\n",
        "        chroma_embedding_functions[collection_name.split(\"_\")[0]],\n",
        "        current_embeddings\n",
        "    )\n",
        "    collections[collection_name] = collection\n",
        "    print(f\"Collection {collection_name} has {collection.count()} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above printout shows the three embedding models applied to the three chunking strategies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have stored all the embeddings in ChromaDB we can test the retrieval process by querying one of our collections and see what the most similar chunks are. Try some different queries and see what the most similar chunks are and whether they make sense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC6YQJ00fakN"
      },
      "outputs": [],
      "source": [
        "selected_collection = collections[\"gte_recursive_1024\"]\n",
        "results = selected_collection.query(\n",
        "    query_texts=[\"Climate Change\"],\n",
        "    n_results=3,\n",
        ")\n",
        "for doc in results[\"documents\"][0]:\n",
        "    print(wrap_text(doc))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTiy5-xzfakN"
      },
      "source": [
        "## Analyzing the Embedding Space\n",
        "\n",
        "To gain a better understandign of how the retrieval process works we will analyze the embedding space. We will start by projecting the embeddings into a 2D space using UMAP. UMAP is a dimensionality reduction technique that is particularly well-suited for visualizing high-dimensional data in a lower-dimensional space. The most notable advantages over other dimensionality reduction techniques are increased speed and better preservation of the data's global structure. We will then use the UMAP embeddings to create a scatter plot of the chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_vectors_from_collection(collection: Collection):\n",
        "    stored_chunks = collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
        "    return np.array(stored_chunks[\"embeddings\"])\n",
        "\n",
        "def get_vectors_by_domain(collection: Collection, domain: str):\n",
        "    stored_chunks = collection.get(include=[\"documents\", \"metadatas\", \"embeddings\"])\n",
        "    metadatas = stored_chunks[\"metadatas\"]\n",
        "    indices = [str(metadata[\"id\"]) for metadata in metadatas if metadata[\"domain\"] == domain]\n",
        "    return collection.get(include=[\"embeddings\"], ids=indices)[\"embeddings\"]\n",
        "\n",
        "def fit_umap(vectors: np.ndarray):\n",
        "    return umap.UMAP().fit(vectors)\n",
        "\n",
        "def project_embeddings(embeddings, umap_transform):\n",
        "    return umap_transform.transform(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4dHra1JfakO"
      },
      "outputs": [],
      "source": [
        "vectors = get_vectors_from_collection(selected_collection)\n",
        "print(f\"Original shape: {vectors.shape}\")\n",
        "umap_transform = fit_umap(vectors)\n",
        "vectors_projections = project_embeddings(vectors, umap_transform)\n",
        "print(f\"Projected shape: {vectors_projections.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dimensions above show how the chunked embeddings with 768 dimensions are reduced to two dimensions for visualization purposes.\n",
        "\n",
        "You can zoom in the plot by clicking and dragging a box around the area you want to zoom in on. You can also reset the plot by double clicking on the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAZLo5U0fakO"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(x=vectors_projections[:, 0], y=vectors_projections[:, 1])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we will color the embeddings by the domain of the article to see if there are any patterns or clusters in the embedding space based on the domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwFmHBKffakO"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "for domain in sample_df[\"domain\"].unique():\n",
        "    domain_vectors = get_vectors_by_domain(selected_collection, domain)\n",
        "    domain_projections = project_embeddings(domain_vectors, umap_transform)\n",
        "    fig.add_trace(go.Scatter(x=domain_projections[:, 0], y=domain_projections[:, 1], mode='markers', marker=dict(size=4), name=domain))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also visualize the retrieval process by plotting the query and the most similar chunks in the embedding space. This will give us a better understanding of how the retrieval process works and how the most similar chunks are found. \n",
        "\n",
        "Note that the UMAP projection uses a metric approach which differs from the approximate nearest neighbor approach used for retrieval. Also don't forget that the embeddings are in a high-dimensional space and we are only visualizing a 2D projection of them so the distances between the points might not be accurate. Try some different queries and see how the most similar chunks are found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_retrieval_results(\n",
        "        query: str,\n",
        "        selected_collection: Collection,\n",
        "        n_results: int = 5\n",
        "):\n",
        "    vectors = get_vectors_from_collection(selected_collection)\n",
        "    umap_transform = fit_umap(vectors)\n",
        "    vectors_projections = project_embeddings(vectors, umap_transform)\n",
        "\n",
        "    query_embedding = selected_collection._embedding_function([query])[0]\n",
        "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
        "    query_projection = project_embeddings(query_embedding, umap_transform)\n",
        "\n",
        "    nearest_neighbors = selected_collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=n_results,\n",
        "    )\n",
        "    neighbor_vectors = selected_collection.get(include=[\"embeddings\"], ids=nearest_neighbors[\"ids\"][0])[\"embeddings\"]\n",
        "    neighbor_projections = project_embeddings(neighbor_vectors, umap_transform)\n",
        "   \n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=vectors_projections[:, 0], y=vectors_projections[:, 1], mode='markers', marker=dict(size=5), name=\"other vectors\"))\n",
        "    fig.add_trace(go.Scatter(x=neighbor_projections[:, 0], y=neighbor_projections[:, 1], mode='markers', marker=dict(size=5, color='orange'), name=\"nearest neighbors\"))\n",
        "    fig.add_trace(go.Scatter(x=query_projection[:, 0], y=query_projection[:, 1], mode='markers', marker=dict(size=10, color='red', symbol='x'), name=\"query\"))\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_retrieval_results(\n",
        "    \"Climate Change\",\n",
        "    selected_collection,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly we will analyze the distribution of the cosine distances between the query and the different chunks. This will give us a better understanding of the cosine distance and show that the distances in the high-dimensional space are not the same as in the 2D projection. Do not confuse the cosine distance with the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). The cosine similarity is the cosine of the angle between two vectors and the cosine distance is 1 minus the cosine similarity so that smaller numbers mean the vectors are more similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_HGmcpBfakO"
      },
      "outputs": [],
      "source": [
        "def cosine_distance(vector1, vector2):\n",
        "    dot_product = np.dot(vector1, vector2.T)\n",
        "    norm_product = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
        "    similarity = dot_product / norm_product\n",
        "    return 1 - similarity\n",
        "\n",
        "def plot_cosine_distances(\n",
        "        query: str,\n",
        "        selected_collection: Collection\n",
        "):\n",
        "    vectors = get_vectors_from_collection(selected_collection)\n",
        "    umap_transform = fit_umap(vectors)\n",
        "    vectors_projections = project_embeddings(vectors, umap_transform)\n",
        "\n",
        "    query_embedding = selected_collection._embedding_function([query])[0]\n",
        "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
        "    query_projection = project_embeddings(query_embedding, umap_transform)\n",
        "\n",
        "    similarities = np.array([cosine_distance(query_embedding, vector) for vector in vectors])\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=vectors_projections[:, 0],\n",
        "        y=vectors_projections[:, 1],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=5,\n",
        "            color=similarities.flatten(),\n",
        "            colorscale='RdBu',\n",
        "            colorbar=dict(title='Cosine Distance')\n",
        "        ),\n",
        "        text=['Cosine Distance: {:.4f}'.format(\n",
        "            sim) for sim in similarities.flatten()],\n",
        "        name='Other Vectors'\n",
        "    ))\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=[query_projection[0][0]], y=[\n",
        "                query_projection[0][1]], mode='markers', marker=dict(size=10, color='black', symbol='x'), text=['Query Vector'], name='Query Vector'))\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_cosine_distances(\n",
        "    \"Climate Change\",\n",
        "    selected_collection,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS7FfQS6fakO"
      },
      "source": [
        "## Putting it all Together\n",
        "\n",
        "Now that we have generated the embeddings and stored them in ChromaDB we can put it all together and create the RAG pipeline. The RAG pipeline consists of the following steps:\n",
        "- **Indexing:** The first step is the preperation we have already done. We have chunked the articles and generated embeddings for the chunks and stored them in ChromaDB, our vector store/index.\n",
        "- **Retrieval:** The next step in the RAG pipeline is to retrieve the most relevant chunks to the user query. This is done by querying the ChromaDB index with the user query and retrieving the most similar chunks.\n",
        "- **Generation:** The next step is to generate a response to the user query. This is done by feeding the retrieved chunks and the user query to the LLM and generating a response.\n",
        "\n",
        "### How does Langchain work?\n",
        "\n",
        "In this notebook we will be using [Langchain](https://www.langchain.com/) to build up our pipeline. You do not need a library like Langchain or [LlamaIndex](https://www.llamaindex.ai/) to build a RAG pipeline, but it can make the process easier. \n",
        "\n",
        "The idea of Langchain and its LCEL (Langchain Expression Language) is very simple. Within the pipeline there are lots of steps that take an input and produce an output. These steps can be chained together to form a pipeline. The LCEL is a simple language that allows you to define these steps and how they are connected. For more technical details on how Langchain works check out the [Langchain Documentation](https://python.langchain.com/v0.1/docs/expression_language/).\n",
        "\n",
        "In simple terms langchain provides an abstraction of a step that has an `invoke` method that takes an input, a dictionary of parameters and returns an output also a dictionary. This allows you to chain together different steps and define how they are connected and also split of chains of steps into separate pipelines.\n",
        "\n",
        "Below you can see an overview of our RAG pipeline:\n",
        "\n",
        "![rag_pipeline](https://miro.medium.com/v2/resize:fit:1400/0*XE_XZ3QrRhlB3b_q.png)\n",
        "\n",
        "And now let's look at the implementation of the RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgwYVtG_fakO"
      },
      "outputs": [],
      "source": [
        "def create_qa_chain(retriever: BaseRetriever):\n",
        "    template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \\\n",
        "    If you don't know the answer, just say that you don't know. Keep the answer concise.\n",
        "\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    rag_prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    rag_chain = RunnableParallel(\n",
        "        {\n",
        "            \"context\": retriever,\n",
        "            \"question\": RunnablePassthrough()\n",
        "        }\n",
        "    ).assign(answer=(\n",
        "         RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
        "            | rag_prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "    ))\n",
        "\n",
        "    return rag_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For Langchain to work with our ChromaDB collections we need to transform the collections into a format that Langchain can work with so called stores and retrievers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collection_to_store(collection_name: str, lc_embedding_model: EmbeddingFunction):\n",
        "    return Chroma(\n",
        "        client=chroma_client,\n",
        "        collection_name=collection_name,\n",
        "        embedding_function=lc_embedding_model,\n",
        "    )\n",
        "\n",
        "def store_to_retriever(store: VectorStore, k: int = 3):\n",
        "    retriever = store.as_retriever(\n",
        "        search_type=\"similarity\", search_kwargs={'k': k}\n",
        "    )\n",
        "    return retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_store = collection_to_store(\"gte_recursive_1024\", embedding_models[\"gte\"])\n",
        "selected_retriever = store_to_retriever(selected_store)\n",
        "selected_retriever.invoke(\"Climate Change\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our retriever we can create our RAG pipeline. Try some different queries and see how the pipeline responds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E9rNMkVfakP"
      },
      "outputs": [],
      "source": [
        "selected_chain = create_qa_chain(selected_retriever)\n",
        "selected_chain.invoke(\"Where are the biggest increases in wildfire smoke exposure in recent years?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chains = {}\n",
        "for collection_name, collection in collections.items():\n",
        "    store = collection_to_store(collection_name, embedding_models[collection_name.split(\"_\")[0]])\n",
        "    retriever = store_to_retriever(store)\n",
        "    chain = create_qa_chain(retriever)\n",
        "    chains[collection_name] = chain\n",
        "\n",
        "chains.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB0DAShJfakP"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Because we have many hyperparameters such as chunk size, prompts etc. to tune and different strategies to try we will use the [RAGAS (RAG Assesment) framework](https://docs.ragas.io/en/stable/) to evaluate our pipeline. RAGAS is a framework that allows you to evaluate your RAG pipeline with an LLM as a judge and other metrics that also utilize embedding models. We will go more into detail on the metrics later on. \n",
        "\n",
        "Before we can start the evaluation we need to define the evaluation questions and their ground truth answers. For this we will use the provided evaluation questions. To increase our question pool we will also generate some additional question and answer pairs based on a random chunk and utilizing the LLM to generate the question and answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "human_eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_eval_answers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    answer_geneation_prompt = \"\"\"Answer the following question based on the article:\n",
        "    Question: {question}\n",
        "    Article: {article}\n",
        "    \"\"\"\n",
        "    answer_generation_chain = ChatPromptTemplate.from_template(answer_geneation_prompt) | llm\n",
        "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        df.at[i, \"ground_truth\"] = answer_generation_chain.invoke({\"question\": row[\"question\"], \"article\": row[\"relevant_section\"]}).content\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if (silver_folder / \"human_eval.csv\").exists():\n",
        "    human_eval_df = pd.read_csv(silver_folder / \"human_eval.csv\")\n",
        "else:\n",
        "    human_eval_df = generate_eval_answers(human_eval_df)\n",
        "    human_eval_df.to_csv(silver_folder / \"human_eval.csv\", index=False)\n",
        "\n",
        "human_eval_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_qa_pairs(documents: List[Document], n: int = 10) -> List[str]:\n",
        "    synthetic_questions = []\n",
        "    documents = np.random.choice(documents, n)\n",
        "\n",
        "    question_generation_prompt = \"\"\"Generate a short and general question based on the following news article:\n",
        "    Article: {article}\n",
        "    \"\"\"\n",
        "    question_generation_chain = ChatPromptTemplate.from_template(question_generation_prompt) | llm\n",
        "\n",
        "    answer_geneation_prompt = \"\"\"Answer the following question based on the article:\n",
        "    Question: {question}\n",
        "    Article: {article}\n",
        "    \"\"\"\n",
        "    answer_generation_chain = ChatPromptTemplate.from_template(answer_geneation_prompt) | llm\n",
        "\n",
        "\n",
        "    for document in tqdm(documents):\n",
        "        element = {}\n",
        "        content = document.page_content\n",
        "        element[\"relevant_section\"] = content\n",
        "        element[\"url\"] = document.metadata[\"url\"]\n",
        "        question = question_generation_chain.invoke({\"article\": content}).content\n",
        "        element[\"question\"] = question\n",
        "        answer = answer_generation_chain.invoke({\"question\": question, \"article\": content}).content\n",
        "        element[\"ground_truth\"] = answer\n",
        "        synthetic_questions.append(element)\n",
        "\n",
        "    return pd.DataFrame(synthetic_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not (silver_folder / \"synthetic_eval.csv\").exists():\n",
        "    synthetic_eval_df = generate_synthetic_qa_pairs(chunks[\"recursive_1024\"], 25)\n",
        "    synthetic_eval_df.to_csv(silver_folder / \"synthetic_eval.csv\", index=False)\n",
        "else:\n",
        "    synthetic_eval_df = pd.read_csv(silver_folder / \"synthetic_eval.csv\", index_col=0)\n",
        "synthetic_eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_length = {\n",
        "    \"human\": human_eval_df[\"question\"].map(len),\n",
        "    \"synthetic\": synthetic_eval_df[\"question\"].map(len)\n",
        "}\n",
        "\n",
        "sns.histplot(question_length, kde=True)\n",
        "plt.title(\"Question Length Distribution\")\n",
        "plt.xlabel(\"Question Length\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_df = pd.concat([human_eval_df, synthetic_eval_df], ignore_index=True)\n",
        "eval_df[\"is_synthetic\"] = eval_df[\"relevant_section\"].isna()\n",
        "eval_df[\"is_synthetic\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have doubled the number of questions and answers. However, we can see that our synthetic questions are slightly longer than the provided questions which could mean that they are slightly easier to answer. This potential bias should be taken into account when evaluating the pipeline.\n",
        "\n",
        "### RAGAS Metrics\n",
        "\n",
        "RAGAS provides a variety of metrics to evaluate the performance of a RAG pipeline. Here are some of the key metrics we will be using and how they are calculated:\n",
        "\n",
        "- [Answer Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html): This metric measures the relevance of the generated answer to the user query. The Answer Relevancy is defined as the mean cosine similartiy of the original question to a number of artifical questions, which where generated (reverse engineered) based on the answer.\n",
        "- [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html): This metric measures the correctness of the generated answer. Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score.\n",
        "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html): This metric measures how well the generated answer is faithful to the retrieved chunks. The generated answer is regarded as faithful if all the claims that are made in the answer can be inferred from the given context. To calculate this a set of claims from the generated answer is first identified. Then each one of these claims are cross checked with given context to determine if it can be inferred from given context or not.\n",
        "- [Context Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/context_relevancy.html): This metric measures the relevance of the retrieved chunks to the user query. Ideally, the retrieved context should exclusively contain essential information to address the provided query. To compute this, we initially estimate the number of sentences within the retrieved context that are relevant for answering the given question and devide it by the total number of sentences in the retrieved context.\n",
        "\n",
        "For this to work we create a test dataset for each of our RAG pipelines that contains the evaluation questions and their ground truth answers. We then run all the questions through our RAG pipeline and store the generated answers and the retrieved chunks. We can then use this test dataset to calculate the RAGAS metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets_folder = gold_folder / \"datasets\"\n",
        "if not datasets_folder.exists():\n",
        "    datasets_folder.mkdir()\n",
        "\n",
        "def get_or_create_eval_dataset(name: str, df: pd.DataFrame, chain: Chain) -> Dataset:\n",
        "    dataset_file = datasets_folder/ f\"{name}_dataset.json\"\n",
        "    if dataset_file.exists():\n",
        "        with open(dataset_file, \"r\") as file:\n",
        "            dataset = Dataset.from_dict(json.load(file))\n",
        "        print(f\"Loaded {name} dataset from {dataset_file}\")\n",
        "    else:\n",
        "        datapoints = {\n",
        "            \"question\": df[\"question\"].tolist(),\n",
        "            \"answer\": [],\n",
        "            \"contexts\": [],\n",
        "            \"ground_truth\": df[\"ground_truth\"].tolist(),\n",
        "            \"context_urls\": []\n",
        "        }\n",
        "        for question in tqdm(datapoints[\"question\"]):\n",
        "            result = chain.invoke(question)\n",
        "            datapoints[\"answer\"].append(result[\"answer\"])\n",
        "            datapoints[\"contexts\"].append([str(doc.page_content) for doc in result[\"context\"]])\n",
        "            datapoints[\"context_urls\"].append([doc.metadata[\"url\"] for doc in result[\"context\"]])\n",
        "        dataset = Dataset.from_dict(datapoints)\n",
        "        with open(dataset_file, \"w\") as file:\n",
        "            json.dump(dataset.to_dict(), file)\n",
        "        print(f\"Saved {name} dataset to {dataset_file}\")\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_folder = gold_folder / \"results\"\n",
        "if not results_folder.exists():\n",
        "    results_folder.mkdir()\n",
        "\n",
        "def get_or_run_llm_eval(name: str, dataset: Dataset, llm_judge_model: LLM) -> pd.DataFrame:\n",
        "    eval_results_file = results_folder / f\"{name}_llm_eval_results.csv\"\n",
        "    if eval_results_file.exists():\n",
        "        eval_results = pd.read_csv(eval_results_file)\n",
        "        print(f\"Loaded {name} evaluation results from {eval_results_file}\")\n",
        "    else:\n",
        "        eval_results = evaluate(dataset,\n",
        "                                metrics=[faithfulness, answer_relevancy, context_relevancy, answer_correctness],\n",
        "                                is_async=True,\n",
        "                                llm=llm_judge_model,\n",
        "                                embeddings=embedding_models[\"gte\"],\n",
        "                                run_config=RunConfig(\n",
        "                                    timeout=60, max_retries=10, max_wait=60, max_workers=8),\n",
        "                                ).to_pandas()\n",
        "        eval_results.to_csv(eval_results_file, index=False)\n",
        "        print(f\"Saved {name} evaluation results to {eval_results_file}\")\n",
        "    return eval_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_llm_eval(name: str, eval_results: pd.DataFrame):\n",
        "    # select only the float64 columns (assuming these are the RAGAS metrics)\n",
        "    ragas_metrics_data = (eval_results\n",
        "                        .select_dtypes(include=[np.float64]))\n",
        "\n",
        "\n",
        "    # boxplot of distributions\n",
        "    sns.boxplot(data=ragas_metrics_data, palette=\"Set2\")\n",
        "    plt.title(f'{name}: Distribution of RAGAS Evaluation Metrics')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # barplot of means\n",
        "    means = ragas_metrics_data.mean()\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(x=means.index, y=means, palette=\"Set2\")\n",
        "    plt.title(f'{name}: Mean of RAGAS Evaluation Metrics')\n",
        "    plt.ylabel('Mean Scores')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_multiple_evals(eval_results: Dict[str, pd.DataFrame]):\n",
        "    # combine the results\n",
        "    full_results = []\n",
        "    for name, results in eval_results.items():\n",
        "        results['name'] = name\n",
        "        full_results.append(results)\n",
        "\n",
        "    full_results = pd.concat(full_results, ignore_index=True)\n",
        "    full_results = full_results.sort_values(by='name')\n",
        "\n",
        "\n",
        "    # select only the float64 columns (assuming these are the RAGAS metrics)\n",
        "    ragas_metrics_data = full_results.select_dtypes(include=[np.float64])\n",
        "    ragas_metrics_data['name'] = full_results['name']\n",
        "    \n",
        "    # boxplot of distributions\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.boxplot(x='variable', y='value', hue='name', data=pd.melt(ragas_metrics_data, id_vars='name'), palette=\"Set2\")\n",
        "    plt.title('Distribution of RAGAS Evaluation Metrics by Model')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Model')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # barplot of means\n",
        "    means = ragas_metrics_data.groupby('name').mean().reset_index()\n",
        "    means_melted = pd.melt(means, id_vars='name')\n",
        "    \n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(x='variable', y='value', hue='name', data=means_melted, palette=\"Set2\")\n",
        "    plt.title('Mean of RAGAS Evaluation Metrics by Model')\n",
        "    plt.ylabel('Mean Scores')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend(title='Model')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_dataset = get_or_create_eval_dataset(\"selected\", eval_df, selected_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_llm_eval_results = get_or_run_llm_eval(\"selected\", selected_dataset, llm)\n",
        "plot_llm_eval(\"selected\", selected_llm_eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = {}\n",
        "for name, chain in chains.items():\n",
        "    datasets[name] = get_or_create_eval_dataset(name, eval_df, chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_results = {}\n",
        "for dataset_name, dataset in datasets.items():\n",
        "    llm_results[dataset_name] = get_or_run_llm_eval(dataset_name, dataset, llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_multiple_evals(llm_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the evaluation we can see that the RAG pipeline using the GTE embedding model by alibaba and recursive chunking with a chunk size of 1024 has the best performance. This is likely due to the fact that the GTE embedding model is the most powerful and the recursive chunking with a chunk size of 1024 provides the most context to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_collection = collections[\"gte_recursive_1024\"]\n",
        "best_store = collection_to_store(\"gte_recursive_1024\", embedding_models[\"gte\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Methods\n",
        "\n",
        "In this final section we will look at some more advanced methods to improve our RAG pipeline and comparing them to our best performing pipeline. \n",
        "\n",
        "### Multi-Querying\n",
        "\n",
        "Multi-querying is a technique that involves querying the retrieval model with multiple questions to retrieve relevant chunks. This approach can enhance the retrieval process by leveraging the diversity of queries to capture a broader range of relevant information. By combining the results from multiple queries, we can potentially improve the quality of the retrieved chunks and, consequently, the generated responses. When creating these additional queries the goal is to create queries that are different from the original query but still relevant to the user's information need, i.e variations of the original query.\n",
        "\n",
        "![multi-querying](https://miro.medium.com/v2/resize:fit:1400/1*wa27F8CbKqYtaRRGyRKy0Q.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_query_variations(query: str, num_additional_queries: int) -> List[str]:\n",
        "    multiquery_prompt = \"\"\"You are an assistant tasked with generating {num_queries} \\\n",
        "    different versions of the given user question to retrieve relevant documents from a vector \\\n",
        "    database. By generating multiple perspectives on the user question and breaking it down, your goal is to help \\\n",
        "    the user overcome some of the limitations of the distance-based similarity search. \\\n",
        "    Provide these alternative questions separated by newlines without any numbering or listing.\n",
        "    Original question: {question}\n",
        "    Alternatives:\n",
        "    \"\"\"\n",
        "\n",
        "    multiquery_chain = ChatPromptTemplate.from_template(multiquery_prompt) | llm\n",
        "    return multiquery_chain.invoke({\"question\": query, \"num_queries\": num_additional_queries}).content.split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_multiquery_retrieval_results(query: str, collection : Collection, num_additional_queries: int = 3, num_results: int = 3):\n",
        "    vectors = get_vectors_from_collection(collection)\n",
        "    umap_transform = fit_umap(vectors)\n",
        "    vectors_projections = project_embeddings(vectors, umap_transform)\n",
        "\n",
        "    query_projections = project_embeddings(collection._embedding_function([query]), umap_transform)\n",
        "\n",
        "    query_variations = generate_query_variations(query, 5)\n",
        "    query_variations_projections = project_embeddings(collection._embedding_function(query_variations), umap_transform)\n",
        "\n",
        "    original_relevant_docs = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=num_results,\n",
        "    )\n",
        "    original_relevant_docs_ids = [item for sublist in original_relevant_docs[\"ids\"] for item in sublist] # flatten\n",
        "    original_relevant_docs_embeddings = collection.get(include=[\"embeddings\"], ids=original_relevant_docs_ids)[\"embeddings\"]\n",
        "    original_relevant_docs_projections = project_embeddings(original_relevant_docs_embeddings, umap_transform)\n",
        "    \n",
        "    additional_relevant_docs = collection.query(\n",
        "        query_texts=query_variations,\n",
        "        n_results=num_results,\n",
        "    )\n",
        "    additional_relevant_docs_ids = [item for sublist in additional_relevant_docs[\"ids\"] for item in sublist] # flatten \n",
        "    # remove duplicates\n",
        "    additional_relevant_docs_ids = list(set(additional_relevant_docs_ids))\n",
        "    # remove the original relevant docs from the additional relevant docs\n",
        "    additional_relevant_docs_ids = [doc_id for doc_id in additional_relevant_docs_ids if doc_id not in original_relevant_docs_ids]\n",
        "    additional_relevant_docs_embeddings = collection.get(include=[\"embeddings\"], ids=additional_relevant_docs_ids)[\"embeddings\"]\n",
        "    additional_relevant_docs_projections = project_embeddings(additional_relevant_docs_embeddings, umap_transform)\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=vectors_projections[:, 0], y=vectors_projections[:, 1], mode='markers', marker=dict(size=5), name=\"other vectors\"))\n",
        "    fig.add_trace(go.Scatter(x=query_projections[:, 0], y=query_projections[:, 1], mode='markers', marker=dict(size=7, color='black', symbol='x'), name=\"original query\"))\n",
        "    fig.add_trace(go.Scatter(x=query_variations_projections[:, 0], y=query_variations_projections[:, 1], mode='markers', marker=dict(size=7, color='red', symbol='x'), name=\"query variations\"))\n",
        "    fig.add_trace(go.Scatter(x=original_relevant_docs_projections[:, 0], y=original_relevant_docs_projections[:, 1], mode='markers', marker=dict(size=7, color='orange'), name=\"original relevant docs\"))\n",
        "    fig.add_trace(go.Scatter(x=additional_relevant_docs_projections[:, 0], y=additional_relevant_docs_projections[:, 1], mode='markers', marker=dict(size=7, color='green'), name=\"additional relevant docs\"))\n",
        "    \n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_multiquery_retrieval_results(\"Climate Change\", selected_collection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA4AAfUXfakS"
      },
      "outputs": [],
      "source": [
        "class MultiQueryRetriever(BaseRetriever):\n",
        "    store: VectorStore\n",
        "    num_additional_queries: int = 3\n",
        "    num_results: int = 3\n",
        "\n",
        "    def _get_query_variations(self, query: str) -> List[str]:\n",
        "       return generate_query_variations(query, self.num_additional_queries)\n",
        "\n",
        "    def _get_relevant_documents(\n",
        "        self, original_query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
        "    ) -> List[Document]:\n",
        "        queries = self._get_query_variations(original_query)\n",
        "        queries.append(original_query)\n",
        "        retriever = store_to_retriever(self.store, k=self.num_results)\n",
        "        relevant_docs = []\n",
        "        for query in queries:\n",
        "            results = retriever.invoke(query, run_manager=run_manager)\n",
        "            # remove duplicates\n",
        "            for res in results:\n",
        "                if res not in relevant_docs:\n",
        "                    relevant_docs.append(res)\n",
        "        return relevant_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdyc8j9vfakS"
      },
      "outputs": [],
      "source": [
        "multiquery_retriever = MultiQueryRetriever(store=best_store, num_additional_queries=3, num_results=3)\n",
        "multiquery_chain = create_qa_chain(multiquery_retriever)\n",
        "multiquery_chain.invoke(\"Where are the biggest increases in wildfire smoke exposure in recent years?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets[\"multiquery\"] = get_or_create_eval_dataset(\"multiquery\", eval_df, multiquery_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_results[\"multiquery\"] = get_or_run_llm_eval(\"multiquery\", datasets[\"multiquery\"], llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strategy_results = {}\n",
        "strategy_results[\"gte_recursive_1024\"] = llm_results[\"gte_recursive_1024\"]\n",
        "strategy_results[\"multiquery\"] = llm_results[\"multiquery\"]\n",
        "plot_multiple_evals(strategy_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqqx_gY2fakS"
      },
      "source": [
        "We can see that on average the answer correctness does slightly increase when using multi-querying. This is likely due to the fact that the retrieval process is more robust and can capture a broader range of relevant information. However, the faithfullness and context_relevancy decrease could be due to the multi-querying introducing more noise into the retrieval process by retrieving more chunks in general and some of them being less relevant.\n",
        "\n",
        "### HyDE - Hypothetical Document Embeddings\n",
        "\n",
        "The idea of the HyDE method is to generate hypothetical documents that are similar to the user query and then retrieve the most similar chunks to these hypothetical documents. This can be useful when the user query is not very specific or when the user query is not very similar to the chunks. The HyDE method can be used to generate hypothetical documents that are more similar to the chunks and therefore improve the retrieval process. Another way to think about it is generating a hypothetical answer and therby reaching an area in the embedding space that is more similar to the actual answer which might not be reachable from the user query.\n",
        "\n",
        "![hyde](https://miro.medium.com/v2/resize:fit:1200/1*vPX3TZXE5NQ0wHjrpI_muw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_hypothetical_document(query: str, num_hypotheses: int) -> List[str]:\n",
        "    hyde_prompt = \"\"\"Please write a news passage about the topic.\n",
        "    Topic: {query}\n",
        "    Passage:\n",
        "    \"\"\"\n",
        "\n",
        "    hyde_chain = ChatPromptTemplate.from_template(hyde_prompt) | llm\n",
        "    hypothetical_documents = [hyde_chain.invoke({\"query\": query}).content for _ in range(num_hypotheses)]\n",
        "    return hypothetical_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_hyde_retrieval_results(query: str, collection : Collection, num_hypo_documents: int = 2, num_results: int = 3):\n",
        "    vectors = get_vectors_from_collection(collection)\n",
        "    umap_transform = fit_umap(vectors)\n",
        "    vectors_projections = project_embeddings(vectors, umap_transform)\n",
        "\n",
        "    query_projections = project_embeddings(collection._embedding_function([query]), umap_transform)\n",
        "\n",
        "    hypothetical_documents = generate_hypothetical_document(query, num_hypo_documents)\n",
        "    query_variations_projections = project_embeddings(collection._embedding_function(hypothetical_documents), umap_transform)\n",
        "\n",
        "    original_relevant_docs = collection.query(\n",
        "        query_texts=[query],\n",
        "        n_results=num_results,\n",
        "    )\n",
        "    original_relevant_docs_ids = [item for sublist in original_relevant_docs[\"ids\"] for item in sublist] # flatten\n",
        "    original_relevant_docs_embeddings = collection.get(include=[\"embeddings\"], ids=original_relevant_docs_ids)[\"embeddings\"]\n",
        "    original_relevant_docs_projections = project_embeddings(original_relevant_docs_embeddings, umap_transform)\n",
        "    \n",
        "    additional_relevant_docs = collection.query(\n",
        "        query_texts=hypothetical_documents,\n",
        "        n_results=num_results,\n",
        "    )\n",
        "    additional_relevant_docs_ids = [item for sublist in additional_relevant_docs[\"ids\"] for item in sublist] # flatten \n",
        "    # remove duplicates\n",
        "    additional_relevant_docs_ids = list(set(additional_relevant_docs_ids))\n",
        "    # remove the original relevant docs from the additional relevant docs\n",
        "    additional_relevant_docs_ids = [doc_id for doc_id in additional_relevant_docs_ids if doc_id not in original_relevant_docs_ids]\n",
        "    additional_relevant_docs_embeddings = collection.get(include=[\"embeddings\"], ids=additional_relevant_docs_ids)[\"embeddings\"]\n",
        "    additional_relevant_docs_projections = project_embeddings(additional_relevant_docs_embeddings, umap_transform)\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=vectors_projections[:, 0], y=vectors_projections[:, 1], mode='markers', marker=dict(size=5), name=\"other vectors\"))\n",
        "    fig.add_trace(go.Scatter(x=query_projections[:, 0], y=query_projections[:, 1], mode='markers', marker=dict(size=7, color='black', symbol='x'), name=\"original query\"))\n",
        "    fig.add_trace(go.Scatter(x=query_variations_projections[:, 0], y=query_variations_projections[:, 1], mode='markers', marker=dict(size=7, color='red', symbol='x'), name=\"hypothetical documents\"))\n",
        "    fig.add_trace(go.Scatter(x=original_relevant_docs_projections[:, 0], y=original_relevant_docs_projections[:, 1], mode='markers', marker=dict(size=7, color='orange'), name=\"original relevant docs\"))\n",
        "    fig.add_trace(go.Scatter(x=additional_relevant_docs_projections[:, 0], y=additional_relevant_docs_projections[:, 1], mode='markers', marker=dict(size=7, color='green'), name=\"additional relevant docs\"))\n",
        "    \n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_hyde_retrieval_results(\"Climate Change\", selected_collection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAZVrwRefakS"
      },
      "outputs": [],
      "source": [
        "class HyDERetriever(BaseRetriever):\n",
        "    store: VectorStore\n",
        "    num_hypo_documents: int = 2\n",
        "    num_results: int = 3\n",
        "\n",
        "    def _get_hypothetical_documents(self, query: str) -> List[str]:\n",
        "        return generate_hypothetical_document(query, self.num_hypo_documents)\n",
        "\n",
        "    def _get_relevant_documents(\n",
        "        self, original_query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
        "    ) -> List[Document]:\n",
        "        hypothetical_documents = self._get_hypothetical_documents(original_query)\n",
        "        hypothetical_documents.append(original_query)\n",
        "        retriever = store_to_retriever(self.store, k=self.num_results)\n",
        "        relevant_docs = []\n",
        "        for query in hypothetical_documents:\n",
        "            results = retriever.invoke(query, run_manager=run_manager)\n",
        "            # remove duplicates\n",
        "            for res in results:\n",
        "                if res not in relevant_docs:\n",
        "                    relevant_docs.append(res)\n",
        "        return relevant_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvk0LsDrfakS"
      },
      "outputs": [],
      "source": [
        "hyde_retriever = HyDERetriever(store=best_store, k=3)\n",
        "hyde_chain = create_qa_chain(hyde_retriever)\n",
        "hyde_chain.invoke(\"Where are the biggest increases in wildfire smoke exposure in recent years?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets[\"hyde\"] = get_or_create_eval_dataset(\"hyde\", eval_df, hyde_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_results[\"hyde\"] = get_or_run_llm_eval(\"hyde\", datasets[\"hyde\"], llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strategy_results[\"hyde\"] = llm_results[\"hyde\"]\n",
        "plot_multiple_evals(strategy_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like with multi-querying we can see that the answer correctness increases when using the HyDE method.\n",
        "\n",
        "### Other Methods\n",
        "\n",
        "There are many other methods that can be used to improve the RAG pipeline. Some of these include:\n",
        "- [Step Back](https://arxiv.org/abs/2310.06117): Where the idea is to take a step back and understand the concepts and context of the user query and then use this information to retrieve the most relevant chunks.\n",
        "- [Hybrid Search](https://medium.com/@zilliz_learn/hybrid-search-combining-text-and-image-for-enhanced-search-capabilities-3d3ce27de326): Where the idea is to not only use semantic search but also lexical search to retrieve the most relevant chunks and combine the results with a re-ranking step. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.system(\"jupyter nbconvert --to html --template pj cleantech_rag.ipynb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
